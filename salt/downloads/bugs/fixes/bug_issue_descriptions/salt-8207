Don't know if it's related to #1863 since it's part of a private network where only 5 nodes are allowed to connect to the salt-master daemon. There are no rejected clients there. Running it under an up to date Ubuntu 12.04.3 on a t1.micro machine from EC2. The packages were installed from the SaltStack PPA.

``` bash
salt@salt01:~$ salt --versions-report
           Salt: 0.17.1
         Python: 2.7.3 (default, Sep 26 2013, 20:03:06)
         Jinja2: 2.6
       M2Crypto: 0.21.1
 msgpack-python: 0.1.10
   msgpack-pure: Not Installed
       pycrypto: 2.4.1
         PyYAML: 3.10
          PyZMQ: 13.0.0
            ZMQ: 3.2.2
salt@salt01:~$ uname -a
Linux salt01 3.2.0-55-virtual #85-Ubuntu SMP Wed Oct 2 12:49:31 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
salt@salt01:~$ cat /etc/issue
Ubuntu 12.04.3 LTS \n \l
```

The memory usage is usually a flat line. Yesterday I installed salt-cloud on this machine for testing purposes. We had our disagreements, therefore I purged the package. This morning I received an alert that the used memory is over 95%. Half an hour later, the OOM killed did its job and the salt-master daemon was gone. Looked at the logs from /var/log/salt. I got nothing.

The salt-cloud removing may be a coincidence, but things started to go south roughly at the time the package was purged.

Here's a Gist with extracted info from the monitoring platform: https://gist.github.com/SaltwaterC/7263387

The images are:
- seven days view over the salt-master memory usage
- one hour view when the OOM killer did its job
- the timeframe for when the things started to go badly

Unfortunately, this only proves that there's an issue. I'll try to find a method to reproduce this in a reliable way.

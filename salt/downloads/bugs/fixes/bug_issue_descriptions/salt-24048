Using latest **_develop**_ branch (post 2015.5) will cause `HTTP 599 ContentLength too long error` with large files (>100MB?). For example - this command with a large file (160MB) will cause the error.:

salt '_develop_' cp.get_url https://s3.amazonaws.com/karpractice/jboss-eap-6.4.0.zip /tmp/blah

Traced back to /salt/utils/http.py line 

```
...
382       result = HTTPClient().fetch(
...
```

This appears to be due to new use of tornado as default http client in http.query and the default  tornado limits for max_buffer_size (100MB?). As a workaround, adding the max_buffer_size argument will allow a larger file to download. An example that allows for 300MB:

```
...
382       result = HTTPClient(max_buffer_size=1024*1024*300).fetch(
...
```

It should be noted that this not only impacts cp.get_url, but modules that use cp.get_url - such as archive.extracted

Perhaps we need to be able to configure a download file size max now? I'm not sure how this impacts memory usage.

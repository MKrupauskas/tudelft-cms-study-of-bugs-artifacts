As we told with @basepi on IRC filling the bug.

I faced the issue running minion on:
- XEN VM
- 2 CPU, 2GB RAM
- master 0.15.1 / Centos 5 / zeromq-2.2.0-4.el5
- minion 0.15.1 / Centos 6 / zeromq3-3.2.3-1.el6.x86_64

starting from some point host started swapping. At the first glance I found ~100 salt-minion processes, like:

USER   PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root      5735 26.0  1.6 707484 32392 ?        Sl   18:48   0:00 /usr/bin/python /usr/bin/salt-minion -d"

Minion log contained:

2013-05-22 15:31:33,033 [salt.minion                                 ][ERROR   ] Exception [Errno 5] Input/output error occurred in scheduled job
2013-05-22 15:31:33,112 [salt.minion                                 ][ERROR   ] Exception [Errno 5] Input/output error occurred in scheduled job
2013-05-22 15:31:33,193 [salt.minion                                 ][ERROR   ] Exception [Errno 5] Input/output error occurred in scheduled job
2013-05-22 15:31:33,272 [salt.minion                                 ][ERROR   ] Exception [Errno 5] Input/output error occurred in scheduled job
2013-05-22 15:31:33,353 [salt.minion                                 ][ERROR   ] Exception [Errno 5] Input/output error occurred in scheduled job
2013-05-22 15:31:33,433 [salt.minion                                 ][ERROR   ] Exception [Errno 5] Input/output error occurred in scheduled job

Apparently issue was related to fast respawn of minion processes

I should mention about similar issue happened a couple days ago. Accidentally minion started to load 100% CPU and log these messages:

2013-05-21 14:01:44,086 [salt.crypt            ][ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate

once per ~100 milliseconds (just like in log above)

The `hash_file` function in `fileclient.py` currently reads in the entire file to be hashed to memory to compute the hash, this causes memory errors when managing large files.

Reading the file in chunks and using `hash.update(chunk)` would solve this problem.

[These lines](https://github.com/saltstack/salt/blob/develop/salt/fileclient.py#L835-L836) causes the problem: 

``` python
with salt.utils.fopen(path, 'rb') as ifile:
    ret['hsum'] = hash_type(ifile.read()).hexdigest()
```

My salt-master runs in an HA cluster and one we were rebuilding the master over the weekend so the salt-master was down for over 48hrs, all of my minions have spawed around 100-150 processes all with separate PIDs but they are all forked from PID 1 as if they are coming from systemd.  I am not sure why I am seeing 100+ minions though, systemd manifest also looks pretty straight forward.  Curious if this is a bug I have hit I wouldn't imagine this to be the normal behavior.

Has anyone seen anything like that with the latest salt?  Any strace shows them in epoll_wait(), resumes then an attempt to open a socket to the master and then fails. The cycle repeats as epoll should in an event loop. 

[pid 32378] <... epoll_wait resumed> {}, 256, 5005) = 0
[pid 32378] socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC, IPPROTO_TCP) = 28
[pid 32378] fcntl(28, F_GETFL)          = 0x2 (flags O_RDWR)
[pid 32378] fcntl(28, F_SETFL, O_RDWR|O_NONBLOCK) = 0
[pid 32378] connect(28, {sa_family=AF_INET, sin_port=htons(4506), sin_addr=inet_addr("10.8.4.97")}, 16) = -1 EINPROGRESS (Operation now in progress)
[pid 32378] epoll_ctl(20, EPOLL_CTL_ADD, 28, {...}) = 0
[pid 32378] epoll_ctl(20, EPOLL_CTL_MOD, 28, {...}) = 0
[pid 32378] epoll_wait(20, {?} 0x7fe192ffb1d0, 256, -1) = 1
[pid 32378] getsockopt(28, SOL_SOCKET, SO_ERROR, [111], [4]) = 0
[pid 32378] epoll_ctl(20, EPOLL_CTL_DEL, 28, {...}) = 0
[pid 32378] close(28)                   = 0
[pid 32378] epoll_wait(20, 

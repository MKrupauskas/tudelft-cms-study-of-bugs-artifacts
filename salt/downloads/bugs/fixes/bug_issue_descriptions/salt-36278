### Description of Issue/Question

I am using a custom returner to return a custom event to the salt master when a scheduled job (via the salt scheduler) fails.  On the master a reactor triggers a runner that sends a slack notification.

One issue I have is that not all jobs have unique names, and when a scheduled job is run, the custom returner is provided a "schedule" value that contains the name of the job (instead of the id which is unique).  I restart the salt master and all minions around midnight PST every night (otherwise there are occasional issues).  One of the things we do via scheduler is apply highstate every 30 minutes except for the 2 hours around midnight PST.  Since the range option on schedule does not support and end time that is less than the start time I have to define two schedule jobs for the highstate, one that runs from just after midnight GMT until one hour before midnight PST, another that runs from one hour after midnight PST to just before midnight GMT.

```
schedule:
  highstate-before-midnight-pst:
    name: highstate
    function: state.apply
    run_on_start: False
    kwargs:
      test: False
    seconds: 1800 {#- 30 minutes #}
    splay: 1200
    range:
      start: 9:00pm
      end: 11:59pm
    returner: schedule-returner
    maxrunning: 3
  highstate-after-midnight-pst:
    name: highstate
    run_on_start: False
    function: state.apply
    kwargs:
      test: False
    seconds: 1800 {#- 30 minutes #}
    splay: 1200
    range:
      start: 12:01am
      end: 7:00pm
    returner: schedule-returner
    maxrunning: 3
```

The first issue is that, in the case of highstate the name of both scheduled jobs must be "highstate" so I can not differentiate between the states (in the case of a failure) with their name because the name is not unique (this is an issue for any scheduled job using `function: state.apply`), only the id is guaranteed to be unique.  But the id is not provided in the data sent to the custom returner so it is not possible to distinguish which exact scheduled job has failed.

The second issue is related to disabling such jobs.  To throttle the alerting, the runner tries to disable the scheduled job that failed so it will not run again until the minion is restarted (thus limiting a specific job failure from a single minion to alerting once a day).  However, since the runner does not know the id of the job that failed, just the name (which is not unique), when it sends the schedule.disable_job command (e.g. `client.cmd(target_minion_id, "schedule.disable_job", [job_name])` where `job_name = "highstate"`) it appears to disable the job with that name which appears first in the pillar.  This may or may not be the right job to disable.  In a case like this (disable by name where name is not unique) a better behavior would be to disable all jobs with that name.
### Versions Report

```
Salt Version:
           Salt: 2015.8.11

Dependency Versions:
         Jinja2: 2.7.2
       M2Crypto: 0.21.1
           Mako: Not Installed
         PyYAML: 3.11
          PyZMQ: 14.7.0
         Python: 2.7.5 (default, Nov 20 2015, 02:00:19)
           RAET: Not Installed
        Tornado: 4.2.1
            ZMQ: 4.0.5
           cffi: Not Installed
       cherrypy: Not Installed
       dateutil: 1.5
          gitdb: Not Installed
      gitpython: Not Installed
          ioflo: Not Installed
        libgit2: Not Installed
        libnacl: Not Installed
   msgpack-pure: Not Installed
 msgpack-python: 0.4.7
   mysql-python: Not Installed
      pycparser: Not Installed
       pycrypto: 2.6.1
         pygit2: Not Installed
   python-gnupg: Not Installed
          smmap: Not Installed
        timelib: Not Installed

System Versions:
           dist: centos 7.2.1511 Core
        machine: x86_64
        release: 3.10.0-229.14.1.el7.x86_64
         system: CentOS Linux 7.2.1511 Core
```
